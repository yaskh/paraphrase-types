{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from scipy.spatial import distance\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(\n",
    "    examples,\n",
    "    sentence1_key,\n",
    "    sentence2_key,\n",
    "    paraphrase_type_id2cls_id,\n",
    "    tokenizer,\n",
    "):\n",
    "    sentence1_key = sentence1_key + \"_tokenized\"\n",
    "    sentence2_key = sentence2_key + \"_tokenized\"\n",
    "\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    tokenized_inputs = tokenizer(*args, truncation=True, is_split_into_words=True, return_offsets_mapping=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "def create_label_maps(etpc):\n",
    "    # Flatten paraphrase_types as list\n",
    "    all_types = {el for sublist in etpc[\"paraphrase_types\"] for el in sublist}\n",
    "\n",
    "    # Download xml with paraphrase types to ids from url https://github.com/venelink/ETPC/blob/master/Corpus/paraphrase_types.xml\n",
    "    url = \"https://raw.githubusercontent.com/venelink/ETPC/master/Corpus/paraphrase_types.xml\"\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.text)\n",
    "\n",
    "    # Get paraphrase types, ids and categories\n",
    "    paraphrase_types = [child.find(\"type_name\").text for child in root]\n",
    "    paraphrase_type_ids = [int(child.find(\"type_id\").text) for child in root]\n",
    "    paraphrase_type_categories = [child.find(\"type_category\").text for child in root]\n",
    "\n",
    "    # Create dictionary with paraphrase type as key and paraphrase type id as value\n",
    "    paraphrase_type2cls_id = dict(zip(paraphrase_types, paraphrase_type_ids))\n",
    "    paraphrase_id2cls_type = dict(zip(paraphrase_type_ids, paraphrase_types))\n",
    "\n",
    "    # Create dictionary with paraphrase type as key and paraphrase type category as value\n",
    "    paraphrase_type_to_category = dict(\n",
    "        zip(paraphrase_types, paraphrase_type_categories)\n",
    "    )\n",
    "\n",
    "    # Add 0 for no paraphrase to all dictionaries\n",
    "    paraphrase_type2cls_id[\"no_paraphrase\"] = 0\n",
    "    paraphrase_id2cls_type[0] = \"no_paraphrase\"\n",
    "    paraphrase_type_to_category[\"no_paraphrase\"] = \"no_paraphrase\"\n",
    "\n",
    "    # Create label2id and id2label for etpc paraphrase_types\n",
    "    label2cls_id = {label: i + 1 for i, label in enumerate(all_types)}\n",
    "    cls_id2label = {i: label for label, i in label2cls_id.items()}\n",
    "\n",
    "    # Add 0 for no paraphrase to all dictionaries\n",
    "    label2cls_id[\"no_paraphrase\"] = 0\n",
    "    cls_id2label[0] = \"no_paraphrase\"\n",
    "\n",
    "    # Create a map from ids to the ones in paraphrase_type_to_id and vice versa\n",
    "    cls_id2paraphrase_type_id = {\n",
    "        i: paraphrase_type2cls_id[cls_id2label[i]] for i in cls_id2label\n",
    "    }\n",
    "    paraphrase_type_id2cls_id = {\n",
    "        paraphrase_type2cls_id[cls_id2label[i]]: i for i in cls_id2label\n",
    "    }\n",
    "\n",
    "    # Create a dictionary that maps ids from label2cls_id to the ones in paraphrase_type_to_id using the type label and vice versa\n",
    "    cls_id2paraphrase_type_id = {\n",
    "        i: paraphrase_type2cls_id[cls_id2label[i]] for i in cls_id2label\n",
    "    }\n",
    "    paraphrase_type_id2cls_id = {\n",
    "        paraphrase_type2cls_id[cls_id2label[i]]: i for i in cls_id2label\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        label2cls_id,\n",
    "        cls_id2label,\n",
    "        paraphrase_type2cls_id,\n",
    "        paraphrase_id2cls_type,\n",
    "        paraphrase_type_to_category,\n",
    "        cls_id2paraphrase_type_id,\n",
    "        paraphrase_type_id2cls_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jpwahle/etpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "sentence1_key = \"sentence1\"\n",
    "sentence2_key = \"sentence2\"\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_bert = AutoModel.from_pretrained(model_path)\n",
    "    return model_bert, tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    label2cls_id,\n",
    "    cls_id2label,\n",
    "    paraphrase_type2cls_id,\n",
    "    paraphrase_id2cls_type,\n",
    "    paraphrase_type_to_category,\n",
    "    cls_id2paraphrase_type_id,\n",
    "    paraphrase_type_id2cls_id,) = create_label_maps(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5801/5801 [00:01<00:00, 4266.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model('/Users/yasir/github/paraphrase-types/out/cls-models/bert-large-uncased-jpwahle/etpc-paraphrase-detection/checkpoint-3045')\n",
    "dataset_tokenized = dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            fn_kwargs={\n",
    "                \"sentence1_key\": sentence1_key,\n",
    "                \"sentence2_key\": sentence2_key,\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"paraphrase_type_id2cls_id\": paraphrase_type_id2cls_id,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 2], [2, 4], [4, 6], [0, 7], [0, 3], [0, 7], [0, 1], [0, 4], [0, 2], [0, 6], [0, 1], [1, 2], [0, 3], [0, 7], [0, 1], [1, 2], [0, 1], [0, 2], [0, 12], [0, 2], [2, 6], [6, 10], [0, 3], [0, 8], [0, 1], [0, 0], [0, 9], [0, 2], [0, 3], [0, 2], [0, 4], [0, 1], [1, 2], [0, 3], [0, 7], [0, 1], [1, 2], [0, 1], [0, 2], [2, 4], [4, 6], [0, 7], [0, 3], [0, 7], [0, 2], [0, 12], [0, 2], [2, 6], [6, 10], [0, 3], [0, 8], [0, 1], [0, 0]]\n",
      "torch.Size([1, 54]) torch.Size([1, 54]) torch.Size([1, 54])\n"
     ]
    }
   ],
   "source": [
    "def encode(input_ids, attention_mask, token_type_ids, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    return outputs\n",
    "print(dataset_tokenized['offset_mapping'][0])\n",
    "attention_mask = torch.tensor([dataset_tokenized['attention_mask'][0]])\n",
    "input_ids = torch.tensor([dataset_tokenized['input_ids'][0]])\n",
    "token_type_ids = torch.tensor([dataset_tokenized['token_type_ids'][0]])\n",
    "\n",
    "print(attention_mask.shape, input_ids.shape, token_type_ids.shape)\n",
    "outputs = encode(input_ids, attention_mask, token_type_ids, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence1', 'sentence2', 'sentence1_tokenized', 'sentence2_tokenized', 'etpc_label', 'mrpc_label', 'negation', 'paraphrase_types', 'paraphrase_type_ids', 'sentence1_segment_location', 'sentence2_segment_location', 'sentence1_segment_location_indices', 'sentence2_segment_location_indices', 'sentence1_segment_text', 'sentence2_segment_text', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
      "    num_rows: 5801\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_tokenized = dataset_tokenized['sentence1_tokenized'][0]\n",
    "sentence2_tokenized = dataset_tokenized['sentence2_tokenized'][0]\n",
    "offsets = dataset_tokenized['offset_mapping'][0]\n",
    "input_ids = dataset_tokenized['input_ids'][0]\n",
    "attention_mask = dataset_tokenized['attention_mask'][0]\n",
    "texts = [dataset_tokenized['sentence1'][0], dataset_tokenized['sentence2'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "Tokens: ['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "target_word = \"Amrozi accused his brother\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i:i+len(target_word_tokens)] == target_word_tokens:\n",
    "        word_start_index = i\n",
    "        word_end_index = i + len(target_word_tokens) - 1\n",
    "        break\n",
    "\n",
    "print(word_start_index, word_end_index)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1053\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[0;32m-> 1053\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5\n",
      "Tokens: ['[CLS]', 'my', 'name', 'is', 'ya', '##sir', '[SEP]']\n",
      "Embedding for 'Yasir': tensor([-0.3394,  0.0585, -0.5342,  ..., -0.0869, -0.2932,  0.1511])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "model, tokenizer = load_model('/Users/yasir/github/paraphrase-types/out/cls-models/bert-large-uncased-jpwahle/etpc-paraphrase-detection/checkpoint-3045')\n",
    "\n",
    "# Encode the sentence using the tokenizer\n",
    "sentence = \"My name is Yasir\"\n",
    "encoded_input = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get the embeddings from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "# The last hidden state of the model\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Convert token IDs back to words to find the index of the target word\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "\n",
    "# Find the indices of the tokens corresponding to the word 'love'\n",
    "target_word = \"Yasir\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "# Get the start and end index of the word \"love\" tokens\n",
    "word_start_index = None\n",
    "word_end_index = None\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i:i+len(target_word_tokens)] == target_word_tokens:\n",
    "        word_start_index = i\n",
    "        word_end_index = i + len(target_word_tokens) - 1\n",
    "        break\n",
    "\n",
    "if word_start_index is None:\n",
    "    raise ValueError(f\"Word '{target_word}' not found in the tokenized sentence.\")\n",
    "\n",
    "print(word_start_index, word_end_index)\n",
    "# Get the embeddings for the subwords of 'love'\n",
    "love_embeddings = last_hidden_states[0, word_start_index:word_end_index+1, :]\n",
    "\n",
    "# Average the embeddings of the subwords to get the final embedding for 'love'\n",
    "love_embedding = love_embeddings.mean(dim=0)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Embedding for '{target_word}': {love_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "target_word = \"Yasir\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
