{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(\n",
    "    examples,\n",
    "    sentence1_key,\n",
    "    sentence2_key,\n",
    "    paraphrase_type_id2cls_id,\n",
    "    tokenizer,\n",
    "):\n",
    "    sentence1_key = sentence1_key + \"_tokenized\"\n",
    "    sentence2_key = sentence2_key + \"_tokenized\"\n",
    "\n",
    "    args = (\n",
    "        (examples[sentence1_key],)\n",
    "        if sentence2_key is None\n",
    "        else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    tokenized_inputs = tokenizer(*args, truncation=True, is_split_into_words=True, return_offsets_mapping=True)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "def create_label_maps(etpc):\n",
    "    # Flatten paraphrase_types as list\n",
    "    all_types = {el for sublist in etpc[\"paraphrase_types\"] for el in sublist}\n",
    "\n",
    "    # Download xml with paraphrase types to ids from url https://github.com/venelink/ETPC/blob/master/Corpus/paraphrase_types.xml\n",
    "    url = \"https://raw.githubusercontent.com/venelink/ETPC/master/Corpus/paraphrase_types.xml\"\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.text)\n",
    "\n",
    "    # Get paraphrase types, ids and categories\n",
    "    paraphrase_types = [child.find(\"type_name\").text for child in root]\n",
    "    paraphrase_type_ids = [int(child.find(\"type_id\").text) for child in root]\n",
    "    paraphrase_type_categories = [child.find(\"type_category\").text for child in root]\n",
    "\n",
    "    # Create dictionary with paraphrase type as key and paraphrase type id as value\n",
    "    paraphrase_type2cls_id = dict(zip(paraphrase_types, paraphrase_type_ids))\n",
    "    paraphrase_id2cls_type = dict(zip(paraphrase_type_ids, paraphrase_types))\n",
    "\n",
    "    # Create dictionary with paraphrase type as key and paraphrase type category as value\n",
    "    paraphrase_type_to_category = dict(\n",
    "        zip(paraphrase_types, paraphrase_type_categories)\n",
    "    )\n",
    "\n",
    "    # Add 0 for no paraphrase to all dictionaries\n",
    "    paraphrase_type2cls_id[\"no_paraphrase\"] = 0\n",
    "    paraphrase_id2cls_type[0] = \"no_paraphrase\"\n",
    "    paraphrase_type_to_category[\"no_paraphrase\"] = \"no_paraphrase\"\n",
    "\n",
    "    # Create label2id and id2label for etpc paraphrase_types\n",
    "    label2cls_id = {label: i + 1 for i, label in enumerate(all_types)}\n",
    "    cls_id2label = {i: label for label, i in label2cls_id.items()}\n",
    "\n",
    "    # Add 0 for no paraphrase to all dictionaries\n",
    "    label2cls_id[\"no_paraphrase\"] = 0\n",
    "    cls_id2label[0] = \"no_paraphrase\"\n",
    "\n",
    "    # Create a map from ids to the ones in paraphrase_type_to_id and vice versa\n",
    "    cls_id2paraphrase_type_id = {\n",
    "        i: paraphrase_type2cls_id[cls_id2label[i]] for i in cls_id2label\n",
    "    }\n",
    "    paraphrase_type_id2cls_id = {\n",
    "        paraphrase_type2cls_id[cls_id2label[i]]: i for i in cls_id2label\n",
    "    }\n",
    "\n",
    "    # Create a dictionary that maps ids from label2cls_id to the ones in paraphrase_type_to_id using the type label and vice versa\n",
    "    cls_id2paraphrase_type_id = {\n",
    "        i: paraphrase_type2cls_id[cls_id2label[i]] for i in cls_id2label\n",
    "    }\n",
    "    paraphrase_type_id2cls_id = {\n",
    "        paraphrase_type2cls_id[cls_id2label[i]]: i for i in cls_id2label\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        label2cls_id,\n",
    "        cls_id2label,\n",
    "        paraphrase_type2cls_id,\n",
    "        paraphrase_id2cls_type,\n",
    "        paraphrase_type_to_category,\n",
    "        cls_id2paraphrase_type_id,\n",
    "        paraphrase_type_id2cls_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jpwahle/etpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence1_tokenized</th>\n",
       "      <th>sentence2_tokenized</th>\n",
       "      <th>etpc_label</th>\n",
       "      <th>mrpc_label</th>\n",
       "      <th>negation</th>\n",
       "      <th>paraphrase_types</th>\n",
       "      <th>paraphrase_type_ids</th>\n",
       "      <th>sentence1_segment_location</th>\n",
       "      <th>sentence2_segment_location</th>\n",
       "      <th>sentence1_segment_location_indices</th>\n",
       "      <th>sentence2_segment_location_indices</th>\n",
       "      <th>sentence1_segment_text</th>\n",
       "      <th>sentence2_segment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_0</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.</td>\n",
       "      <td>[Amrozi, accused, his, brother, ,, whom, he, called, ``, the, witness, '', ,, of, deliberately, distorting, his, evidence, .]</td>\n",
       "      <td>[Referring, to, him, as, only, ``, the, witness, '', ,, Amrozi, accused, his, brother, of, deliberately, distorting, his, evidence, .\\n]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Same Polarity Substitution (habitual), Same Polarity Substitution (contextual), Change of order, Addition/Deletion, Identity]</td>\n",
       "      <td>[5, 6, 26, 25, 29]</td>\n",
       "      <td>[26, 26, 26, 26, 0, 5, 0, 6, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]</td>\n",
       "      <td>[6, 5, 5, 0, 25, 0, 0, 0, 0, 0, 26, 26, 26, 26, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[5], [7], [0, 1, 2, 3], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]</td>\n",
       "      <td>[[1, 2], [0], [10, 11, 12, 13], [4]]</td>\n",
       "      <td>[whom, called, Amrozi accused his brother, `` the witness '' , of deliberately distorting his evidence .]</td>\n",
       "      <td>[to him, Referring, Amrozi accused his brother, only, `` the witness '' , of deliberately distorting his evidence .\\n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_1</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.</td>\n",
       "      <td>[Yucaipa, owned, Dominick, 's, before, selling, the, chain, to, Safeway, in, 1998, for, $, 2.5, billion, .]</td>\n",
       "      <td>[Yucaipa, bought, Dominick, 's, in, 1995, for, $, 693, million, and, sold, it, to, Safeway, for, $, 1.8, billion, in, 1998, .\\n]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_2</td>\n",
       "      <td>They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.</td>\n",
       "      <td>On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.</td>\n",
       "      <td>[They, had, published, an, advertisement, on, the, Internet, on, June, 10, ,, offering, the, cargo, for, sale, ,, he, added, .]</td>\n",
       "      <td>[On, June, 10, ,, the, ship, 's, owners, had, published, an, advertisement, on, the, Internet, ,, offering, the, explosives, for, sale, .\\n]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Same Polarity Substitution (contextual), Same Polarity Substitution (contextual), Change of order, Addition/Deletion, Identity]</td>\n",
       "      <td>[6, 6, 26, 25, 29]</td>\n",
       "      <td>[6, 0, 0, 0, 0, 0, 0, 0, 26, 26, 26, 0, 0, 0, 6, 0, 0, 25, 25, 25, 0]</td>\n",
       "      <td>[26, 26, 26, 26, 6, 6, 6, 6, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 6, 25, 25, 25]</td>\n",
       "      <td>[[0], [14], [8, 9, 10], [17, 18, 19]]</td>\n",
       "      <td>[[4, 5, 6, 7], [18], [0, 1, 2, 3], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21]]</td>\n",
       "      <td>[They, cargo, on June 10, , he added, had published an advertisement on the Internet , offering the for sale .]</td>\n",
       "      <td>[the ship 's owners, explosives, On June 10 ,, had published an advertisement on the Internet , offering the for sale .\\n]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_3</td>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, or 4.4%, at A$4.56, having earlier set a record high of A$4.57.</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A$4.57.</td>\n",
       "      <td>[Around, 0335, GMT, ,, Tab, shares, were, up, 19, cents, ,, or, 4.4, %, ,, at, A, $, 4.56, ,, having, earlier, set, a, record, high, of, A, $, 4.57, .]</td>\n",
       "      <td>[Tab, shares, jumped, 20, cents, ,, or, 4.6, %, ,, to, set, a, record, closing, high, at, A, $, 4.57, .\\n]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5_4</td>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.</td>\n",
       "      <td>[The, stock, rose, $, 2.11, ,, or, about, 11, percent, ,, to, close, Friday, at, $, 21.51, on, the, New, York, Stock, Exchange, .]</td>\n",
       "      <td>[PG, &amp;, E, Corp., shares, jumped, $, 1.63, or, 8, percent, to, $, 21.03, on, the, New, York, Stock, Exchange, on, Friday, .\\n]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[Same Polarity Substitution (contextual), Same Polarity Substitution (habitual), Same Polarity Substitution (contextual), Synthetic/analytic substitution, Change of order, Addition/Deletion, Identity, Non-paraphrase, Non-paraphrase, Non-paraphrase, Punctuation changes]</td>\n",
       "      <td>[6, 5, 6, 11, 26, 25, 29, 30, 30, 30, 21]</td>\n",
       "      <td>[6, 6, 5, 29, 30, 0, 29, 25, 30, 29, 0, 6, 6, 26, 6, 29, 30, 29, 29, 29, 29, 29, 29, 29]</td>\n",
       "      <td>[30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]</td>\n",
       "      <td>[[0, 1], [2], [11, 12, 14], [13], [13], [7], [3, 6, 9, 15, 17, 18, 19, 20, 21, 22, 23], [4], [8], [16]]</td>\n",
       "      <td>[[0, 1, 2, 3, 4], [5], [11], [20, 21], [20, 21], [6, 8, 10, 12, 14, 15, 16, 17, 18, 19, 22], [7], [9], [13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]]</td>\n",
       "      <td>[The stock, rose, to close at, Friday, Friday, about, $ or percent $ on the New York Stock Exchange ., 2.11, 11, 21.51, The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .]</td>\n",
       "      <td>[PG &amp; E Corp. shares, jumped, to, on Friday, on Friday, $ or percent $ on the New York Stock Exchange .\\n, 1.63, 8, 21.03, PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\\n]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  \\\n",
       "0  1_0   \n",
       "1  2_1   \n",
       "2  3_2   \n",
       "3  4_3   \n",
       "4  5_4   \n",
       "\n",
       "                                                                                                       sentence1  \\\n",
       "0             Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.   \n",
       "1                         Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.   \n",
       "2         They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, or 4.4%, at A$4.56, having earlier set a record high of A$4.57.   \n",
       "4           The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.   \n",
       "\n",
       "                                                                                                         sentence2  \\\n",
       "0      Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.   \n",
       "1              Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.   \n",
       "2  On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.   \n",
       "3                                     Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A$4.57.   \n",
       "4                  PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.   \n",
       "\n",
       "                                                                                                                                       sentence1_tokenized  \\\n",
       "0                            [Amrozi, accused, his, brother, ,, whom, he, called, ``, the, witness, '', ,, of, deliberately, distorting, his, evidence, .]   \n",
       "1                                              [Yucaipa, owned, Dominick, 's, before, selling, the, chain, to, Safeway, in, 1998, for, $, 2.5, billion, .]   \n",
       "2                          [They, had, published, an, advertisement, on, the, Internet, on, June, 10, ,, offering, the, cargo, for, sale, ,, he, added, .]   \n",
       "3  [Around, 0335, GMT, ,, Tab, shares, were, up, 19, cents, ,, or, 4.4, %, ,, at, A, $, 4.56, ,, having, earlier, set, a, record, high, of, A, $, 4.57, .]   \n",
       "4                       [The, stock, rose, $, 2.11, ,, or, about, 11, percent, ,, to, close, Friday, at, $, 21.51, on, the, New, York, Stock, Exchange, .]   \n",
       "\n",
       "                                                                                                                            sentence2_tokenized  \\\n",
       "0      [Referring, to, him, as, only, ``, the, witness, '', ,, Amrozi, accused, his, brother, of, deliberately, distorting, his, evidence, .\\n]   \n",
       "1              [Yucaipa, bought, Dominick, 's, in, 1995, for, $, 693, million, and, sold, it, to, Safeway, for, $, 1.8, billion, in, 1998, .\\n]   \n",
       "2  [On, June, 10, ,, the, ship, 's, owners, had, published, an, advertisement, on, the, Internet, ,, offering, the, explosives, for, sale, .\\n]   \n",
       "3                                    [Tab, shares, jumped, 20, cents, ,, or, 4.6, %, ,, to, set, a, record, closing, high, at, A, $, 4.57, .\\n]   \n",
       "4                [PG, &, E, Corp., shares, jumped, $, 1.63, or, 8, percent, to, $, 21.03, on, the, New, York, Stock, Exchange, on, Friday, .\\n]   \n",
       "\n",
       "   etpc_label  mrpc_label  negation  \\\n",
       "0           1           1         0   \n",
       "1           0           0         0   \n",
       "2           1           1         0   \n",
       "3           0           0         0   \n",
       "4           0           1         0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                paraphrase_types  \\\n",
       "0                                                                                                                                                 [Same Polarity Substitution (habitual), Same Polarity Substitution (contextual), Change of order, Addition/Deletion, Identity]   \n",
       "1                                                                                                                                                                                                                                                                             []   \n",
       "2                                                                                                                                               [Same Polarity Substitution (contextual), Same Polarity Substitution (contextual), Change of order, Addition/Deletion, Identity]   \n",
       "3                                                                                                                                                                                                                                                                             []   \n",
       "4  [Same Polarity Substitution (contextual), Same Polarity Substitution (habitual), Same Polarity Substitution (contextual), Synthetic/analytic substitution, Change of order, Addition/Deletion, Identity, Non-paraphrase, Non-paraphrase, Non-paraphrase, Punctuation changes]   \n",
       "\n",
       "                         paraphrase_type_ids  \\\n",
       "0                         [5, 6, 26, 25, 29]   \n",
       "1                                         []   \n",
       "2                         [6, 6, 26, 25, 29]   \n",
       "3                                         []   \n",
       "4  [6, 5, 6, 11, 26, 25, 29, 30, 30, 30, 21]   \n",
       "\n",
       "                                                                      sentence1_segment_location  \\\n",
       "0                       [26, 26, 26, 26, 0, 5, 0, 6, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]   \n",
       "1                                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2                          [6, 0, 0, 0, 0, 0, 0, 0, 26, 26, 26, 0, 0, 0, 6, 0, 0, 25, 25, 25, 0]   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4       [6, 6, 5, 29, 30, 0, 29, 25, 30, 29, 0, 6, 6, 26, 6, 29, 30, 29, 29, 29, 29, 29, 29, 29]   \n",
       "\n",
       "                                                                     sentence2_segment_location  \\\n",
       "0                             [6, 5, 5, 0, 25, 0, 0, 0, 0, 0, 26, 26, 26, 26, 0, 0, 0, 0, 0, 0]   \n",
       "1                            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2           [26, 26, 26, 26, 6, 6, 6, 6, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 6, 25, 25, 25]   \n",
       "3                               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]   \n",
       "\n",
       "                                                                        sentence1_segment_location_indices  \\\n",
       "0                                     [[5], [7], [0, 1, 2, 3], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]]   \n",
       "1                                                                                                       []   \n",
       "2                                                                    [[0], [14], [8, 9, 10], [17, 18, 19]]   \n",
       "3                                                                                                       []   \n",
       "4  [[0, 1], [2], [11, 12, 14], [13], [13], [7], [3, 6, 9, 15, 17, 18, 19, 20, 21, 22, 23], [4], [8], [16]]   \n",
       "\n",
       "                                                                                                                                                                 sentence2_segment_location_indices  \\\n",
       "0                                                                                                                                                              [[1, 2], [0], [10, 11, 12, 13], [4]]   \n",
       "1                                                                                                                                                                                                []   \n",
       "2                                                                                                            [[4, 5, 6, 7], [18], [0, 1, 2, 3], [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20, 21]]   \n",
       "3                                                                                                                                                                                                []   \n",
       "4  [[0, 1, 2, 3, 4], [5], [11], [20, 21], [20, 21], [6, 8, 10, 12, 14, 15, 16, 17, 18, 19, 22], [7], [9], [13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]]   \n",
       "\n",
       "                                                                                                                                                                                                               sentence1_segment_text  \\\n",
       "0                                                                                                                           [whom, called, Amrozi accused his brother, `` the witness '' , of deliberately distorting his evidence .]   \n",
       "1                                                                                                                                                                                                                                  []   \n",
       "2                                                                                                                     [They, cargo, on June 10, , he added, had published an advertisement on the Internet , offering the for sale .]   \n",
       "3                                                                                                                                                                                                                                  []   \n",
       "4  [The stock, rose, to close at, Friday, Friday, about, $ or percent $ on the New York Stock Exchange ., 2.11, 11, 21.51, The stock rose $ 2.11 , or about 11 percent , to close Friday at $ 21.51 on the New York Stock Exchange .]   \n",
       "\n",
       "                                                                                                                                                                                                               sentence2_segment_text  \n",
       "0                                                                                                              [to him, Referring, Amrozi accused his brother, only, `` the witness '' , of deliberately distorting his evidence .\\n]  \n",
       "1                                                                                                                                                                                                                                  []  \n",
       "2                                                                                                          [the ship 's owners, explosives, On June 10 ,, had published an advertisement on the Internet , offering the for sale .\\n]  \n",
       "3                                                                                                                                                                                                                                  []  \n",
       "4  [PG & E Corp. shares, jumped, to, on Friday, on Friday, $ or percent $ on the New York Stock Exchange .\\n, 1.63, 8, 21.03, PG & E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .\\n]  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': {0: '1_0'}, 'sentence1': {0: 'Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.'}, 'sentence2': {0: 'Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.'}, 'sentence1_tokenized': {0: array(['Amrozi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called',\n",
      "       '``', 'the', 'witness', \"''\", ',', 'of', 'deliberately',\n",
      "       'distorting', 'his', 'evidence', '.'], dtype=object)}, 'sentence2_tokenized': {0: array(['Referring', 'to', 'him', 'as', 'only', '``', 'the', 'witness',\n",
      "       \"''\", ',', 'Amrozi', 'accused', 'his', 'brother', 'of',\n",
      "       'deliberately', 'distorting', 'his', 'evidence', '.\\n'],\n",
      "      dtype=object)}, 'etpc_label': {0: 1}, 'mrpc_label': {0: 1}, 'negation': {0: 0}, 'paraphrase_types': {0: array(['Same Polarity Substitution (habitual)',\n",
      "       'Same Polarity Substitution (contextual)', 'Change of order',\n",
      "       'Addition/Deletion', 'Identity'], dtype=object)}, 'paraphrase_type_ids': {0: array(['5', '6', '26', '25', '29'], dtype=object)}, 'sentence1_segment_location': {0: array([26, 26, 26, 26,  0,  5,  0,  6, 25, 25, 25, 25, 25, 25, 25, 25, 25,\n",
      "       25, 25], dtype=int32)}, 'sentence2_segment_location': {0: array([ 6,  5,  5,  0, 25,  0,  0,  0,  0,  0, 26, 26, 26, 26,  0,  0,  0,\n",
      "        0,  0,  0], dtype=int32)}, 'sentence1_segment_location_indices': {0: array([array([5], dtype=int32), array([7], dtype=int32),\n",
      "       array([0, 1, 2, 3], dtype=int32),\n",
      "       array([ 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18], dtype=int32)],\n",
      "      dtype=object)}, 'sentence2_segment_location_indices': {0: array([array([1, 2], dtype=int32), array([0], dtype=int32),\n",
      "       array([10, 11, 12, 13], dtype=int32), array([4], dtype=int32)],\n",
      "      dtype=object)}, 'sentence1_segment_text': {0: array(['whom', 'called', 'Amrozi accused his brother',\n",
      "       \"`` the witness '' , of deliberately distorting his evidence .\"],\n",
      "      dtype=object)}, 'sentence2_segment_text': {0: array(['to him', 'Referring', 'Amrozi accused his brother', 'only',\n",
      "       \"`` the witness '' , of deliberately distorting his evidence .\\n\"],\n",
      "      dtype=object)}}\n",
      "sentence1_tokenized\n",
      "['Amrozi' 'accused' 'his' 'brother' ',' 'whom' 'he' 'called' '``' 'the'\n",
      " 'witness' \"''\" ',' 'of' 'deliberately' 'distorting' 'his' 'evidence' '.']\n",
      "19\n",
      "\n",
      "sentence1_segment_location_indices\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "row = df.iloc[[0]]\n",
    "print(row.to_dict())\n",
    "\n",
    "sentence1_segment_location = row['sentence1_segment_location'].values[0].flatten()\n",
    "sentence1_segment_location_indices = row['sentence1_segment_location_indices'].values[0].flatten()\n",
    "print('sentence1_tokenized')\n",
    "print(row['sentence1_tokenized'].values[0])\n",
    "print(len(row['sentence1_tokenized'].values[0]))\n",
    "print()\n",
    "print('sentence1_segment_location_indices')\n",
    "sentence1_segment_location_indices = np.concatenate(sentence1_segment_location_indices).flatten()\n",
    "\n",
    "print(len(sentence1_segment_location_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "sentence1_key = \"sentence1\"\n",
    "sentence2_key = \"sentence2\"\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    tokenizer_bert = AutoTokenizer.from_pretrained(model_path)\n",
    "    model_bert = AutoModel.from_pretrained(model_path)\n",
    "    return model_bert, tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    label2cls_id,\n",
    "    cls_id2label,\n",
    "    paraphrase_type2cls_id,\n",
    "    paraphrase_id2cls_type,\n",
    "    paraphrase_type_to_category,\n",
    "    cls_id2paraphrase_type_id,\n",
    "    paraphrase_type_id2cls_id,) = create_label_maps(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5801/5801 [00:01<00:00, 4595.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model('/Users/yasir/github/paraphrase-types/out/cls-models/bert-large-uncased-jpwahle/etpc-paraphrase-detection/checkpoint-3045')\n",
    "dataset_tokenized = dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            fn_kwargs={\n",
    "                \"sentence1_key\": sentence1_key,\n",
    "                \"sentence2_key\": sentence2_key,\n",
    "                \"tokenizer\": tokenizer,\n",
    "                \"paraphrase_type_id2cls_id\": paraphrase_type_id2cls_id,\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 2], [2, 4], [4, 6], [0, 7], [0, 3], [0, 7], [0, 1], [0, 4], [0, 2], [0, 6], [0, 1], [1, 2], [0, 3], [0, 7], [0, 1], [1, 2], [0, 1], [0, 2], [0, 12], [0, 2], [2, 6], [6, 10], [0, 3], [0, 8], [0, 1], [0, 0], [0, 9], [0, 2], [0, 3], [0, 2], [0, 4], [0, 1], [1, 2], [0, 3], [0, 7], [0, 1], [1, 2], [0, 1], [0, 2], [2, 4], [4, 6], [0, 7], [0, 3], [0, 7], [0, 2], [0, 12], [0, 2], [2, 6], [6, 10], [0, 3], [0, 8], [0, 1], [0, 0]]\n",
      "torch.Size([1, 54]) torch.Size([1, 54]) torch.Size([1, 54])\n"
     ]
    }
   ],
   "source": [
    "def encode(input_ids, attention_mask, token_type_ids, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    return outputs\n",
    "\n",
    "# print(dataset_tokenized['offset_mapping'][0])\n",
    "# attention_mask = torch.tensor([dataset_tokenized['attention_mask'][0]])\n",
    "# input_ids = torch.tensor([dataset_tokenized['input_ids'][0]])\n",
    "# token_type_ids = torch.tensor([dataset_tokenized['token_type_ids'][0]])\n",
    "\n",
    "# print(attention_mask.shape, input_ids.shape, token_type_ids.shape)\n",
    "# outputs = encode(input_ids, attention_mask, token_type_ids, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['idx', 'sentence1', 'sentence2', 'sentence1_tokenized', 'sentence2_tokenized', 'etpc_label', 'mrpc_label', 'negation', 'paraphrase_types', 'paraphrase_type_ids', 'sentence1_segment_location', 'sentence2_segment_location', 'sentence1_segment_location_indices', 'sentence2_segment_location_indices', 'sentence1_segment_text', 'sentence2_segment_text', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'],\n",
      "    num_rows: 5801\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_tokenized = dataset_tokenized['sentence1_tokenized'][0]\n",
    "sentence2_tokenized = dataset_tokenized['sentence2_tokenized'][0]\n",
    "offsets = dataset_tokenized['offset_mapping'][0]\n",
    "input_ids = dataset_tokenized['input_ids'][0]\n",
    "attention_mask = dataset_tokenized['attention_mask'][0]\n",
    "texts = [dataset_tokenized['sentence1'][0], dataset_tokenized['sentence2'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 6\n",
      "Tokens: ['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "target_word = \"Amrozi accused his brother\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i:i+len(target_word_tokens)] == target_word_tokens:\n",
    "        word_start_index = i\n",
    "        word_end_index = i + len(target_word_tokens) - 1\n",
    "        break\n",
    "\n",
    "print(word_start_index, word_end_index)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence2 segment location: category of paraphrase types \n",
    "## sentence1_segment_location_indices sentence1_segment_text sentence2_segment_location_indices sentence2_segment_text are useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "1 3\n",
      "Tokens: ['[CLS]', 'am', '##ro', '##zi', 'accused', 'his', 'brother', ',', 'whom', 'he', 'called', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]', 'referring', 'to', 'him', 'as', 'only', '`', '`', 'the', 'witness', \"'\", \"'\", ',', 'am', '##ro', '##zi', 'accused', 'his', 'brother', 'of', 'deliberately', 'di', '##stor', '##ting', 'his', 'evidence', '.', '[SEP]']\n",
      "Embedding for 'Amrozi': tensor([ 0.0899, -0.8350, -0.2770,  ...,  0.2921,  0.8338,  0.4639])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "# # Load pre-trained model tokenizer (vocabulary)\n",
    "# # model, tokenizer = load_model('/Users/yasir/github/paraphrase-types/out/cls-models/bert-large-uncased-jpwahle/etpc-paraphrase-detection/checkpoint-3045')\n",
    "\n",
    "# sentence1_tokenized_key = sentence1_key\n",
    "# sentence2_tokenized_key = sentence2_key\n",
    "\n",
    "# # Encode the sentence using the tokenizer\n",
    "# row = df.iloc[[0]]\n",
    "sentence1 = row[\"sentence1\"].values[0]\n",
    "print(sentence1)\n",
    "# sentence2 = row[sentence2_tokenized_key].values[0]\n",
    "# print(dataset_tokenized[1])\n",
    "\n",
    "\n",
    "offsets = dataset_tokenized['offset_mapping'][0]\n",
    "input_ids = dataset_tokenized['input_ids'][0]\n",
    "attention_mask = dataset_tokenized['attention_mask'][0]\n",
    "token_type_ids = dataset_tokenized['token_type_ids'][0]\n",
    "\n",
    "input_ids = torch.tensor([input_ids])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "token_type_ids = torch.tensor([token_type_ids])\n",
    "\n",
    "encoded_input_manual = BatchEncoding({\n",
    "    'input_ids': input_ids,\n",
    "    'token_type_ids': token_type_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "})\n",
    "\n",
    "encoded_input = tokenizer(sentence1, return_tensors='pt')\n",
    "print(type(encoded_input))\n",
    "print(type(encoded_input_manual))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input_manual)\n",
    "\n",
    "# # The last hidden state of the model\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Reverse tokenization to get the tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input_manual['input_ids'][0])\n",
    "\n",
    "# # Find the indices of the tokens corresponding to the word 'love'\n",
    "target_word = \"Amrozi\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)\n",
    "\n",
    "# # Get the start and end index of the word \"love\" tokens\n",
    "word_start_index = None\n",
    "word_end_index = None\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    if tokens[i:i+len(target_word_tokens)] == target_word_tokens:\n",
    "        word_start_index = i\n",
    "        word_end_index = i + len(target_word_tokens) - 1\n",
    "        break\n",
    "\n",
    "if word_start_index is None:\n",
    "    raise ValueError(f\"Word '{target_word}' not found in the tokenized sentence.\")\n",
    "\n",
    "print(word_start_index, word_end_index)\n",
    "# Get the embeddings for the subwords of 'love'\n",
    "love_embeddings = last_hidden_states[0, word_start_index:word_end_index+1, :]\n",
    "\n",
    "# Average the embeddings of the subwords to get the final embedding for 'love'\n",
    "love_embedding = love_embeddings.mean(dim=0)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Embedding for '{target_word}': {love_embedding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
       "          2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
       "          3436,  2010,  3350,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BatchEncoding\n",
    "BatchEncoding({\n",
    "    'input_ids': torch.tensor([[101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102]]),\n",
    "    'token_type_ids': torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
    "    'attention_mask': torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "love_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "target_word = \"Yasir\"\n",
    "target_word_tokens = tokenizer.tokenize(target_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
